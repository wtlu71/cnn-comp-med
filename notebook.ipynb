{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b867135",
      "metadata": {
        "id": "0b867135"
      },
      "source": [
        "# Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7837677e",
      "metadata": {
        "id": "7837677e"
      },
      "outputs": [],
      "source": [
        "# installs- comment out if good\n",
        "\n",
        "!git clone https://github.com/wtlu71/cnn-comp-med.git\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_rel, f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "repo_path = '/content/cnn-comp-med'\n",
        "# Add to Python path\n",
        "sys.path.append(repo_path)\n",
        "# imports\n",
        "from my_scripts.test import potato\n",
        "from my_scripts.my_models import SmallCNN, SmallMLP,LargeCNN\n",
        "from my_scripts.dataset_loading import H5Dataset\n",
        "from my_scripts.utils import run_epoch\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader, Subset,TensorDataset\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "weights_folder = \"weights\"\n",
        "dataset_folder = os.path.join(os.getcwd(),\"data\")\n",
        "external_dataset_folder = \"breast-histopathology-images-subset\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown"
      ],
      "metadata": {
        "id": "gvPOdjkbE5dJ"
      },
      "id": "gvPOdjkbE5dJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(weights_folder,exist_ok=True)\n",
        "os.makedirs(external_dataset_folder,exist_ok=True)\n",
        "download = True # set true if need to download\n",
        "if download:\n",
        "  !gdown --folder --remaining-ok https://drive.google.com/drive/folders/1EnF-qAa2JMD66KRQCIs-xOUmWiXiDA11 -O weights #weights folder storing train,test,val stratified idx\n",
        "  !gdown --folder --remaining-ok https://drive.google.com/drive/folders/14EiLf9I6FGopdydykxPLmVXsiltzBGDH -O data #saved data for deterministic results\n",
        "  !gdown --id 1lAtil7jdHGi9MM6d3WbxALmYYWVC_2R5 -O breast-histopathology-images-subset.zip #external testing subset\n",
        "  !unzip -q breast-histopathology-images-subset.zip"
      ],
      "metadata": {
        "id": "u5kltBwLFcVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fddcab6-247c-46c2-e056-8b552673f4a5"
      },
      "id": "u5kltBwLFcVV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1TGWX-NN_KmssOnxQucKMhV_t_Fbxobsm .ipynb_checkpoints\n",
            "Retrieving folder 18Lyiz4SQdaDn0Ot0oQj4lN9NKf5qkd_e resnet\n",
            "Processing file 1Z72UXI0XmKiYEEQJDAyV3LFp15_mG4pM resnet.pth\n",
            "Processing file 1HncVXpZ74ubG0dDDgOJ2mwZL8ANnMiCH efficientNet.pth\n",
            "Processing file 19nhMhct1JG_61LIP5YLLEURSLT0i1Xkh LargeCNN_subset.pth\n",
            "Processing file 1SXSm5-C9mwdvmPzfgOlcqj0UipK17PWg ResNet_subset.pth\n",
            "Processing file 1XDoT88p-K5nU-gwBq-26c0OCOM9Nrzyp ResNet.pth\n",
            "Processing file 14o0LoXRmt-YvXEE1fIqXUlvKmxoFtSxw SmallCNN_subset.pth\n",
            "Processing file 1_CjNFjxdXJztdQAb6ASvm_mPQluqU_Jk SmallMLP_subset.pth\n",
            "Processing file 1HkfPOp6pRIM6aYzYTyV7jDjRgueM5PjH test.idx.npy\n",
            "Processing file 1j9CoCJtBHgbnvM4ZrT2mz4wqYQ341Hoz test.npy\n",
            "Processing file 1QXXpAsAGwGknjcOl-j8-TjALetQObNdn train.idx.npy\n",
            "Processing file 1Yl_L14-Q-tGEOEa0KOZwxOnGGulPxHtC train.npy\n",
            "Processing file 1HLwctzyNbIAUcoIpwvh7RXNbA0f27S8V val.idx.npy\n",
            "Processing file 1krzZg7q3pTGPbILC9U9nt4YCFblIRme_ val.npy\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1Z72UXI0XmKiYEEQJDAyV3LFp15_mG4pM\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "Retrieving folder contents\n",
            "Processing file 1wktatS9YtMBVp3urugdvENGXW379gIDu external_test_dataset.pt\n",
            "Processing file 1UcS1NTgqGpPjJ6k3JDvKRlCu7mlByHFq test_dataset.pt\n",
            "Processing file 1krWzeegke_H03xvs4y_2yXc5pdhR592f train_dataset.pt\n",
            "Processing file 1zKccFzoX0e5L5jYuWjF-pOApCBj0ZUJG val_dataset.pt\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1wktatS9YtMBVp3urugdvENGXW379gIDu\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1lAtil7jdHGi9MM6d3WbxALmYYWVC_2R5\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "replace breast-histopathology-images-subset/1/10264_idx5_x801_y1651_class1.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d56a87d",
      "metadata": {
        "id": "9d56a87d"
      },
      "source": [
        "Get train/val/test data from Zenodo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4ba219",
      "metadata": {
        "id": "ec4ba219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72acffbe-c34e-491a-f57d-3b0346894650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "\n",
            "12/12 17:22:27 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\u001b[0m\n",
            "12/12 17:22:29 [\u001b[1;32mNOTICE\u001b[0m] Shutdown sequence commencing... Press Ctrl-C again for emergency shutdown.\n",
            "\n",
            "12/12 17:22:29 [\u001b[1;32mNOTICE\u001b[0m] Download GID#359719237273b2d6 not complete: /content/data/train_x.h5.gz\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "359719|\u001b[1;34mINPR\u001b[0m|    32MiB/s|/content/data/train_x.h5.gz\n",
            "\n",
            "Status Legend:\n",
            "(INPR):download in-progress.\n",
            "\n",
            "aria2 will resume download if the transfer is restarted.\n",
            "If there are any errors, then see the log file. See '-l' option in help/man page for details.\n",
            "\n",
            "gzip: data/train_x.h5.gz: invalid compressed data--format violated\n",
            "--2025-12-12 17:22:34--  https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 188.185.48.75, 137.138.52.235, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21378 (21K) [application/octet-stream]\n",
            "Saving to: ‘data/train_y.h5.gz’\n",
            "\n",
            "data/train_y.h5.gz  100%[===================>]  20.88K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-12-12 17:22:34 (531 KB/s) - ‘data/train_y.h5.gz’ saved [21378/21378]\n",
            "\n",
            "gzip: data/train_y.h5 already exists; do you wish to overwrite (y or n)? ^C\n",
            "\n",
            "12/12 17:22:36 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "12/12 17:22:36 [\u001b[1;32mNOTICE\u001b[0m] Emergency shutdown sequence commencing...\n",
            "\n",
            "12/12 17:22:36 [\u001b[1;32mNOTICE\u001b[0m] Download GID#b69890561cf5186c not complete: /content/data/valid_x.h5.gz\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b69890|\u001b[1;34mINPR\u001b[0m|       0B/s|/content/data/valid_x.h5.gz\n",
            "\n",
            "Status Legend:\n",
            "(INPR):download in-progress.\n",
            "\n",
            "aria2 will resume download if the transfer is restarted.\n",
            "If there are any errors, then see the log file. See '-l' option in help/man page for details.\n",
            "gzip: data/valid_x.h5.gz: No such file or directory\n",
            "--2025-12-12 17:22:36--  https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_y.h5.gz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 188.185.48.75, 137.138.52.235, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3038 (3.0K) [application/octet-stream]\n",
            "Saving to: ‘data/valid_y.h5.gz’\n",
            "\n",
            "data/valid_y.h5.gz    0%[                    ]       0  --.-KB/s               ^C\n",
            "\n",
            "gzip: data/valid_y.h5.gz: unexpected end of file\n",
            "\n",
            "12/12 17:22:36 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "12/12 17:22:37 [\u001b[1;32mNOTICE\u001b[0m] Emergency shutdown sequence commencing...\n",
            "\n",
            "12/12 17:22:37 [\u001b[1;32mNOTICE\u001b[0m] Download GID#6f286d3d9f98880e not complete: /content/data/test_x.h5.gz\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "6f286d|\u001b[1;34mINPR\u001b[0m|    34MiB/s|/content/data/test_x.h5.gz\n",
            "\n",
            "Status Legend:\n",
            "(INPR):download in-progress.\n",
            "\n",
            "aria2 will resume download if the transfer is restarted.\n",
            "If there are any errors, then see the log file. See '-l' option in help/man page for details.\n",
            "gzip: data/test_x.h5 already exists; do you wish to overwrite (y or n)? "
          ]
        }
      ],
      "source": [
        "os.makedirs(dataset_folder,exist_ok=True)\n",
        "# !apt-get install -y wget2\n",
        "if download:\n",
        "  !apt-get install -y aria2\n",
        "\n",
        "  # !aria2c -x 16 -s 16 -k 1M -c -j 1 \"URL\" -o output_filename\n",
        "  #train\n",
        "  !aria2c -x 16 -s 16 -k 1M -c -j 1 \"https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1\" -o data/train_x.h5.gz\n",
        "  !gunzip data/train_x.h5.gz\n",
        "\n",
        "  !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1 -O data/train_y.h5.gz\n",
        "  !gunzip data/train_y.h5.gz\n",
        "\n",
        "  # val\n",
        "  !aria2c -x 16 -s 16 -k 1M -c -j 1 \"https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_x.h5.gz?download=1\" -o data/valid_x.h5.gz\n",
        "  !gunzip data/valid_x.h5.gz\n",
        "\n",
        "  !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_y.h5.gz?download=1 -O data/valid_y.h5.gz\n",
        "  !gunzip data/valid_y.h5.gz\n",
        "\n",
        "  # test\n",
        "  !aria2c -x 16 -s 16 -k 1M -c -j 1 \"https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1\" -o data/test_x.h5.gz\n",
        "  !gunzip data/test_x.h5.gz\n",
        "\n",
        "  !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_y.h5.gz?download=1 -O data/test_y.h5.gz\n",
        "  !gunzip data/test_y.h5.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130649a2",
      "metadata": {
        "id": "130649a2"
      },
      "outputs": [],
      "source": [
        "# define transforms\n",
        "\n",
        "# is there a way to get this programmatically from the data or not worth it\n",
        "IMG_SIZE = 96\n",
        "# BATCH_SIZE = 2048\n",
        "BATCH_SIZE = 1024\n",
        "# Training transforms with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transforms (no augmentation)\n",
        "eval_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "print(\"Data transforms defined\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path):\n",
        "  dataset_dict = torch.load(path)\n",
        "  dataset = TensorDataset(dataset_dict['images'],dataset_dict['labels'])\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def save_dataset(dataset,save_path):\n",
        "  all_images = []\n",
        "  all_labels = []\n",
        "\n",
        "  for img, label in dataset:\n",
        "      all_images.append(img)\n",
        "      all_labels.append(label)\n",
        "\n",
        "  save_dict = {\n",
        "      \"images\": torch.stack(all_images),\n",
        "      \"labels\": torch.tensor(all_labels),\n",
        "  }\n",
        "\n",
        "  torch.save(save_dict, save_path)"
      ],
      "metadata": {
        "id": "XSeshGau3qvy"
      },
      "id": "XSeshGau3qvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a869510",
      "metadata": {
        "id": "2a869510"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dataset paths- Colab virtual session\n",
        "load_saved = True\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "if load_saved:\n",
        "  train_path = os.path.join(dataset_folder,\"train_dataset.pt\")\n",
        "  test_path = os.path.join(dataset_folder,\"test_dataset.pt\")\n",
        "  val_path = os.path.join(dataset_folder,\"val_dataset.pt\")\n",
        "  external_dataset_path = os.path.join(dataset_folder,\"external_test_dataset.pt\")\n",
        "  train_dataset = load_dataset(train_path)\n",
        "  val_dataset = load_dataset(test_path)\n",
        "  test_dataset = load_dataset(val_path)\n",
        "  external_test_dataset = load_dataset(external_dataset_path)\n",
        "\n",
        "else:\n",
        "  train_img_h5_path = os.path.join(dataset_folder,\"train_x.h5\")\n",
        "  train_label_h5_path = os.path.join(dataset_folder,\"train_y.h5\")\n",
        "\n",
        "  val_img_h5_path = os.path.join(dataset_folder,\"valid_x.h5\")\n",
        "  val_label_h5_path = os.path.join(dataset_folder,\"valid_y.h5\")\n",
        "\n",
        "  test_img_h5_path = os.path.join(dataset_folder,\"test_x.h5\")\n",
        "  test_label_h5_path = os.path.join(dataset_folder,\"test_y.h5\")\n",
        "\n",
        "\n",
        "  train_subset_size = 50000 #keeping compute restraints in mind\n",
        "  eval_subset_size = 5000\n",
        "  use_precomputed_indices = True\n",
        "\n",
        "  train_dataset = H5Dataset(train_img_h5_path,train_label_h5_path,transform=train_transforms)\n",
        "  train_labels = np.array([label for _, label in train_dataset])\n",
        "\n",
        "  val_dataset = H5Dataset(val_img_h5_path,val_label_h5_path,transform=train_transforms)\n",
        "  val_labels = np.array([label for _, label in val_dataset])\n",
        "\n",
        "  test_dataset = H5Dataset(test_img_h5_path,test_label_h5_path,transform=train_transforms)\n",
        "  test_labels = np.array([label for _, label in test_dataset])\n",
        "\n",
        "  if use_precomputed_indices:\n",
        "    train_idx = np.load(os.path.join(weights_folder,f\"train.npy\"))\n",
        "    val_idx = np.load(os.path.join(weights_folder,f\"val.npy\"))\n",
        "    test_idx = np.load(os.path.join(weights_folder,f\"test.npy\"))\n",
        "    test_idx = np.load(os.path.join(weights_folder,f\"test.npy\"))\n",
        "  else:\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_subset_size, random_state=RANDOM_STATE)\n",
        "    train_idx, _ = next(sss.split(np.zeros(len(train_labels)), train_labels))  # indices for stratified subset\n",
        "    np.save(os.path.join(weights_folder,f\"train.npy\"),train_idx)\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=eval_subset_size, random_state=RANDOM_STATE)\n",
        "    val_idx, _ = next(sss.split(np.zeros(len(val_labels)), val_labels))  # indices for stratified subset\n",
        "    np.save(os.path.join(weights_folder,f\"val.npy\"),val_idx)\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=eval_subset_size, random_state=RANDOM_STATE)\n",
        "    test_idx, _ = next(sss.split(np.zeros(len(test_labels)), test_labels))  # indices for stratified subset\n",
        "    np.save(os.path.join(weights_folder,f\"test.npy\"),test_idx)\n",
        "\n",
        "  train_dataset = Subset(train_dataset, train_idx)\n",
        "  val_dataset = Subset(val_dataset, val_idx)\n",
        "  test_dataset = Subset(test_dataset, test_idx)\n",
        "  #external is already stratified\n",
        "  external_test_dataset = datasets.ImageFolder(external_dataset_folder,transform=eval_transforms)\n",
        "  #ensure reproducibility of same stratified dataset used across runs\n",
        "  save_dataset(train_dataset,\"train_dataset.pt\")\n",
        "  save_dataset(val_dataset,\"val_dataset.pt\")\n",
        "  save_dataset(test_dataset,\"test_dataset.pt\")\n",
        "  save_dataset(external_test_dataset,\"external_test_dataset.pt\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,pin_memory=pin_memory)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,pin_memory=pin_memory)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,pin_memory=pin_memory)\n",
        "external_test_loader = DataLoader(external_test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,pin_memory=pin_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2f0bdd",
      "metadata": {
        "id": "ea2f0bdd"
      },
      "outputs": [],
      "source": [
        "# define/load models\n",
        "num_classes = 2\n",
        "mlpmodel = SmallMLP(size=((IMG_SIZE,IMG_SIZE))).to(device)\n",
        "smallcnnmodel = SmallCNN(channels=(32,64,128)).to(device)\n",
        "largecnnmodel = LargeCNN(channels=(32,64,128,64,64,64)).to(device)\n",
        "\n",
        "resnetmodel = resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "# resnetmodel = resnet50(weights = None).to(device)\n",
        "resnetmodel.fc = nn.Linear(resnetmodel.fc.in_features, num_classes)\n",
        "# wresnetmodel = models.wide_resnet50_2(pretrained=True).to(device)\n",
        "# wresnetmodel.fc = nn.Linear(wresnetmodel.fc.in_features, num_classes)\n",
        "\n",
        "modellist = [mlpmodel, smallcnnmodel, resnetmodel]\n",
        "modeldict = {\n",
        "    \"SmallMLP\": mlpmodel,\n",
        "    \"SmallCNN\": smallcnnmodel,\n",
        "    \"LargeCNN\": largecnnmodel,\n",
        "    \"ResNet\": resnetmodel\n",
        "}\n",
        "\n",
        "# training hyperparameters\n",
        "# can change learning rate or use scheduler\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LEARNING_RATE = 3e-4\n",
        "# finetune with less epochs to avoid forgetting\n",
        "# EPOCHS = 15\n",
        "EPOCHS = 1\n",
        "PATIENCE = EPOCHS\n",
        "# patience- number of epochs the model continues after no improvement in validation loss\n",
        "\n",
        "# consider other loss functions https://neptune.ai/blog/pytorch-loss-functions\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# don't change adam, never change dude\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sJyz8cYMfFN7",
      "metadata": {
        "id": "sJyz8cYMfFN7"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fd1d2e",
      "metadata": {
        "id": "c1fd1d2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "historylist = []\n",
        "stopepochs = []\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "    print(f\"Training {modelname}...\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),weight_decay=WEIGHT_DECAY, lr=LEARNING_RATE)\n",
        "\n",
        "    # try to integrate into wandb instead of storing this so we can have a pretty dashboard?\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"train_auc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"val_auc\": []\n",
        "    }\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_state = None\n",
        "    bad_epochs = 0\n",
        "\n",
        "    print(\"\\nStarting training...\\n\")\n",
        "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
        "        tr_loss, tr_acc, _, _, tr_auc,_ = run_epoch(train_loader, model, criterion, optimizer=optimizer, train=True, device=device)\n",
        "        va_loss, va_acc, va_sens, va_spec, va_auc,_ = run_epoch(val_loader, model, criterion, optimizer=None, train=False,device=device)\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"val_loss\"].append(va_loss)\n",
        "        history[\"train_auc\"].append(tr_auc)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "        history[\"val_auc\"].append(va_auc)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}: \"\n",
        "            f\"train_loss={tr_loss:.4f} \"\n",
        "            f\"val_loss={va_loss:.4f} \"\n",
        "            f\"val_acc={va_acc:.3f} \"\n",
        "            f\"val_auc={va_auc:.3f}\")\n",
        "\n",
        "        if va_auc > best_auc + 1e-4:\n",
        "            best_auc = va_auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            bad_epochs = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= PATIENCE:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_state is not None:\n",
        "        weights_path = os.path.join(weights_folder,f\"{modelname}_subset.pth\")\n",
        "        torch.save(best_state,weights_path)\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f\"\\nRestored best model (val_auc={best_auc:.4f})\")\n",
        "\n",
        "\n",
        "    plt.plot(history[\"train_loss\"],label=\"train_loss\")\n",
        "    plt.plot(history[\"val_loss\"],label=\"val_loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"CrossEntropyLoss\")\n",
        "    plt.legend()\n",
        "    plt.title(modelname)\n",
        "    plt.show()\n",
        "    plt.plot(history[\"train_acc\"],label=\"train_acc\")\n",
        "    plt.plot(history[\"val_acc\"],label=\"val_acc\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(modelname)\n",
        "    plt.show()\n",
        "    plt.plot(history[\"train_auc\"],label=\"train_auc\")\n",
        "    plt.plot(history[\"val_auc\"],label=\"val_auc\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.show()\n",
        "\n",
        "    historylist.append(history)\n",
        "    stopepochs.append(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYLkfJ6Hsddk",
      "metadata": {
        "id": "wYLkfJ6Hsddk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "history_path = os.path.join(weights_folder,\"history.json\")\n",
        "with open(history_path,'w') as f:\n",
        "  json.dump(historylist,f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvb6NJEmv90P"
      },
      "id": "hvb6NJEmv90P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LKp3AZEgwZVs",
      "metadata": {
        "id": "LKp3AZEgwZVs"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"train_loss\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"CrossEntropyLoss\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Train Loss\")\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P9R1yi8Tx9id",
      "metadata": {
        "id": "P9R1yi8Tx9id"
      },
      "outputs": [],
      "source": [
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"val_loss\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"CrossEntropyLoss\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Val Loss\")\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RyKVs2Cty2IG",
      "metadata": {
        "id": "RyKVs2Cty2IG"
      },
      "outputs": [],
      "source": [
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"train_acc\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Train Accuracy\")\n",
        "  plt.ylim([0,1])\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IFxm7CtizT60",
      "metadata": {
        "id": "IFxm7CtizT60"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XcK2boAAy_D-",
      "metadata": {
        "id": "XcK2boAAy_D-"
      },
      "outputs": [],
      "source": [
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"val_acc\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Valid Accuracy\")\n",
        "  plt.ylim([0,1])\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1qLYGozLGR",
      "metadata": {
        "id": "5c1qLYGozLGR"
      },
      "outputs": [],
      "source": [
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"train_auc\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"AUC\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Train AUC\")\n",
        "  plt.ylim([0,1])\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AyonuTVkzEgh",
      "metadata": {
        "id": "AyonuTVkzEgh"
      },
      "outputs": [],
      "source": [
        "for i,history in enumerate(historylist):\n",
        "  modelname = list(modeldict.keys())[i]\n",
        "  plt.plot(history[\"val_auc\"],label=modelname)\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"AUC\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Val AUC\")\n",
        "  plt.ylim([0,1])\n",
        "  # plt.show()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e442430",
      "metadata": {
        "id": "1e442430"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# finally, evaluate on test set\n",
        "# also do saliency\n",
        "\n",
        "# model_attr_scores = []\n",
        "# model_attr_channels = []\n",
        "# model_preds = []\n",
        "# model_targets = []\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  weights_path = os.path.join(weights_folder,f\"{modelname}_subset.pth\")\n",
        "  model_state_dict = torch.load(weights_path)\n",
        "  model.load_state_dict(model_state_dict)\n",
        "  print(f\"\\nEvaluating {modelname}...\")\n",
        "  model.to(device)\n",
        "  _, va_acc, va_sens, va_spec, va_auc,cm = run_epoch(test_loader, model, criterion, train=False, device=device)\n",
        "  print(f\"Final Validation Performance:\")\n",
        "  print(f\"  AUC:         {va_auc:.4f}\")\n",
        "  print(f\"  Accuracy:    {va_acc:.4f}\")\n",
        "  print(f\"  Sensitivity: {va_sens:.4f}\")\n",
        "  print(f\"  Specificity: {va_spec:.4f}\")\n",
        "  class_names = [\"Normal\", \"Cancer\"]\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                display_labels=class_names)\n",
        "  disp.plot(cmap='Blues', values_format='d')\n",
        "  plt.title(f\"Confusion Matrix {modelname} Internal test set\")\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "    weights_path = os.path.join(weights_folder,f\"{modelname}_subset.pth\")\n",
        "    model_state_dict = torch.load(weights_path)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    print(f\"\\nEvaluating {modelname}...\")\n",
        "    model.to(device)\n",
        "    _, va_acc, va_sens, va_spec, va_auc,cm = run_epoch(external_test_loader, model, criterion, train=False, device=device)\n",
        "    print(f\"Final Validation Performance:\")\n",
        "    print(f\"  AUC:         {va_auc:.4f}\")\n",
        "    print(f\"  Accuracy:    {va_acc:.4f}\")\n",
        "    print(f\"  Sensitivity: {va_sens:.4f}\")\n",
        "    print(f\"  Specificity: {va_spec:.4f}\")\n",
        "    class_names = [\"Normal\", \"Cancer\"]\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                  display_labels=class_names)\n",
        "    disp.plot(cmap='Blues', values_format='d')\n",
        "    plt.title(f\"Confusion Matrix {modelname} External test set\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4Pk1aCvT87Xl"
      },
      "id": "4Pk1aCvT87Xl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating and saliency\n",
        "!pip install torch torchvision captum pillow matplotlib\n",
        "# !pip install captum\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from captum.attr import Saliency\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model_attr_scores = []\n",
        "model_attr_channels = []\n",
        "model_preds = []\n",
        "model_targets = []\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  weights_path = os.path.join(weights_folder,f\"{modelname}_subset.pth\")\n",
        "  model_state_dict = torch.load(weights_path)\n",
        "  model.load_state_dict(model_state_dict)\n",
        "  print(f\"\\nEvaluating saliency for {modelname}...\")\n",
        "  model.to(device)\n",
        "  print(f\"Saliency for {modelname}\")\n",
        "  model.eval()\n",
        "  saliency = Saliency(model)\n",
        "\n",
        "  attr_scores_list = []\n",
        "  attr_channels_list = []\n",
        "  pred_list = []\n",
        "  target_list = []\n",
        "\n",
        "  for images, targets in test_loader:\n",
        "    images = images.to(device, non_blocking=True)\n",
        "    targets = targets.to(device, non_blocking=True)\n",
        "    images.requires_grad_()\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "      logits = model(images)\n",
        "      # loss = criterion(logits, targets)\n",
        "      pred_classes = logits.argmax(dim=1)\n",
        "      attr = saliency.attribute(images, target=pred_classes)\n",
        "      # attr = attr.abs().cpu().detach().numpy()\n",
        "      attr = attr.abs()\n",
        "      attr_scores = attr.max(dim=1)[0]    # (batch_size, H, W)\n",
        "      attr_channel = attr.argmax(dim=1)   # (batch_size, H, W)\n",
        "    pred_list.append(pred_classes.cpu())\n",
        "    target_list.append(targets.cpu())\n",
        "    attr_scores_list.append(attr_scores)\n",
        "    attr_channels_list.append(attr_channel)\n",
        "\n",
        "  all_attr_scores = torch.cat(attr_scores_list, dim=0)\n",
        "  all_attr_channels = torch.cat(attr_channels_list, dim=0)\n",
        "  model_attr_scores.append(all_attr_scores)\n",
        "  model_attr_channels.append(all_attr_channels)\n",
        "  model_preds.append(torch.cat(pred_list, dim=0))\n",
        "  model_targets.append(torch.cat(target_list, dim=0))"
      ],
      "metadata": {
        "id": "9FZSQ2wkLMk-"
      },
      "id": "9FZSQ2wkLMk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "center_size = 32\n",
        "pool_center_mask = torch.zeros((3, 3)).bool()\n",
        "pool_center_mask[1, 1] = True\n",
        "\n",
        "center_mask = torch.zeros((96, 96)).bool()\n",
        "center_mask[32:64, 32:64] = True\n",
        "\n",
        "all_sal_results = []\n",
        "\n",
        "# assess the saliency score maps for each model\n",
        "for i, (saliencies, imp_channels) in enumerate(zip(model_attr_scores, model_attr_channels)):\n",
        "  # correct shape for avgpool\n",
        "  sal = saliencies.unsqueeze(1)\n",
        "  avgpool_sal = F.avg_pool2d(sal, kernel_size = center_size, stride = center_size).squeeze(1)\n",
        "  center_avg = avgpool_sal[:, pool_center_mask].view(-1) # center\n",
        "  outer_avg = avgpool_sal[:, ~pool_center_mask].view(len(avgpool_sal), 8).mean(dim=1) # outer\n",
        "  center_focused = center_avg > outer_avg # vector for each image\n",
        "  ratio = center_avg/outer_avg\n",
        "  print(f\"length of centeravg is {center_avg.shape}\")\n",
        "  print(f\"length of outeravg is {outer_avg.shape}\")\n",
        "  print(f\"length of centerfocused is {center_focused.shape}\")\n",
        "  print(f\"length of ratio is {ratio.shape}\")\n",
        "\n",
        "  # now get most frequent channel in center and in outer\n",
        "  imp_channels_center = imp_channels[:, center_mask]\n",
        "  imp_channels_outer = imp_channels[:, ~center_mask]\n",
        "  imp_channel_center = torch.mode(imp_channels_center.reshape(len(imp_channels), -1), dim=1).values\n",
        "  imp_channel_outer = torch.mode(imp_channels_outer.reshape(len(imp_channels), -1), dim=1).values\n",
        "\n",
        "  for j in range(len(center_avg)):\n",
        "    row = {\n",
        "        \"modelname\": list(modeldict.keys())[i],\n",
        "        \"image index\": j,\n",
        "        \"center\": center_avg[j].item(),\n",
        "        \"outer\": outer_avg[j].item(),\n",
        "        \"center focused\": center_focused[j].item(),\n",
        "        \"center ratio\": ratio[j].item(),\n",
        "        \"important channel center\": imp_channel_center[j].item(),\n",
        "        \"important channel outer\": imp_channel_outer[j].item(),\n",
        "    }\n",
        "    all_sal_results.append(row)\n",
        "    # sal_results = {\n",
        "    #     \"model name\": [list(modeldict.keys())[i]]*len(center_avg.cpu()), # repeat model name for each image\n",
        "    #     \"image index\":\n",
        "    #     \"center\": center_avg.cpu(),\n",
        "    #     \"outer\": outer_avg.cpu(),\n",
        "    #     \"center focused\": center_focused.cpu()\n",
        "    # }\n",
        "    # all_sal_results.append(sal_results)"
      ],
      "metadata": {
        "id": "AQUFUUVimjmV"
      },
      "id": "AQUFUUVimjmV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bar plot- four groups two bars each- for each model, avg. center score vs. avg outer score\n",
        "sal_df = pd.DataFrame(all_sal_results)\n",
        "# melt the center and outer columns together\n",
        "sal_melt_df = sal_df.melt(id_vars=[\"modelname\", \"image index\", \"center focused\"], value_vars=[\"center\", \"outer\"], value_name=\"saliency\", var_name=\"region\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "p = sns.barplot(data=sal_melt_df, x=\"modelname\", y=\"saliency\", hue=\"region\", errorbar=\"sd\")\n",
        "p.set(ylabel=\"Avg Saliency Score\", title=\"Center vs. Outer Saliency by Model\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, fmt='%.3f', padding=-10)\n",
        "plt.show()\n",
        "# paired t-test of center vs. outer for each model\n",
        "models = sal_df[\"modelname\"].unique()\n",
        "for model in models:\n",
        "    sub = sal_df[sal_df[\"modelname\"] == model]\n",
        "    tval, pval = ttest_rel(sub[\"center\"], sub[\"outer\"])\n",
        "    print(f\"{model}: t={tval:.3f}, p={pval:.4f}\")\n",
        "\n",
        "# plot center vs. outer focus ratio for each model\n",
        "p = sns.barplot(data=sal_df, x=\"modelname\", y=\"center ratio\")\n",
        "p.set(ylabel=\"Center vs. Outer Saliency Ratio\", title=\"Center vs. Outer Saliency Ratio by Model\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, fmt='%.3f', padding=-10)\n",
        "plt.show()\n",
        "#1-way ANOVA and Tukey post-hoc for significant difference in center focus b/w models\n",
        "groups = [sal_df[sal_df[\"modelname\"] == model][\"center ratio\"]\n",
        "          for model in models]\n",
        "Fval, pval = f_oneway(*groups)\n",
        "print(f\"ANOVA: F = {Fval:.4f}, p = {pval:.4e}\")\n",
        "tukey = pairwise_tukeyhsd(endog=sal_df[\"center ratio\"], groups=sal_df[\"modelname\"], alpha=0.05)\n",
        "print(tukey)\n",
        "\n",
        "# count how many images were center-focused vs. not\n",
        "hue_order = [True, False]\n",
        "p = sns.countplot(data=sal_df, x=\"modelname\", hue=\"center focused\", hue_order=hue_order)\n",
        "p.set(ylabel=\"Number of Images Center-Focused\", title=\"Count of Images Where Model Focused on Center\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, padding=-10)\n",
        "plt.show()\n",
        "\n",
        "# confusion matrix- 2x4- center-focused or not on one axis, TP, TN, FP, FN on the other\n",
        "sal_df[\"Pred Class\"] = torch.cat(model_preds, dim=0).cpu().numpy()\n",
        "sal_df[\"Target Class\"] = torch.cat(model_targets, dim=0).cpu().numpy()\n",
        "\n",
        "sal_df[\"Class Result\"] = -1\n",
        "\n",
        "sal_df.loc[(sal_df[\"Pred Class\"] == 1) & (sal_df[\"Target Class\"] == 1), \"Class Result\"] = 0 # TP\n",
        "sal_df.loc[(sal_df[\"Pred Class\"] == 0) & (sal_df[\"Target Class\"] == 0), \"Class Result\"] = 1 # TN\n",
        "sal_df.loc[(sal_df[\"Pred Class\"] == 1) & (sal_df[\"Target Class\"] == 0), \"Class Result\"] = 2 # FP\n",
        "sal_df.loc[(sal_df[\"Pred Class\"] == 0) & (sal_df[\"Target Class\"] == 1), \"Class Result\"] = 3 # FN\n",
        "\n",
        "# check to make sure there's only 0, 1, 2, 3 and no -1 in \"Class Result\"\n",
        "print(sal_df[\"Class Result\"].unique())\n",
        "\n",
        "# convert center focused to bool\n",
        "sal_df[\"Center-Focused?\"] = sal_df[\"center focused\"].astype(int)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
        "\n",
        "# plot confusion matrices\n",
        "for ax, modelname in zip(axes, modeldict.keys()):\n",
        "  print(\"confusing\", modelname)\n",
        "  model_df = sal_df[sal_df[\"modelname\"] == modelname]\n",
        "  # sanity check\n",
        "  print(f\"{model_df['Center-Focused?'].sum()} center-focused images\")\n",
        "  print(f\"{(model_df['Class Result'] == 0).sum()} true positives\")\n",
        "  print(f\"{(model_df[\"Class Result\"] == 1).sum()} true negatives\")\n",
        "  print(f\"{(model_df['Class Result'] == 2).sum()} false positives\")\n",
        "  print(f\"{(model_df['Class Result'] == 3).sum()} false negatives\")\n",
        "  cm = confusion_matrix(model_df[\"Center-Focused?\"], model_df[\"Class Result\"])\n",
        "  # plot heatmap\n",
        "  s = sns.heatmap(\n",
        "    cm[:2], annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False,\n",
        "    xticklabels=[\"True Positive\", \"True Negative\", \"False Positive\", \"False Negative\"],\n",
        "    yticklabels=[\"Not Center-Focused\", \"Center-Focused\"]\n",
        "  )\n",
        "  ax.set_title(f\"{modelname}\", fontsize=12, fontweight=\"bold\")\n",
        "  ax.set_xlabel(\"Result\", fontsize=10)\n",
        "  ax.set_ylabel(\"Saliency\", fontsize=10)\n",
        "plt.tight_layout() # adjust spacing\n",
        "plt.show()\n",
        "\n",
        "# plot saliency ratio by class result and model\n",
        "# make a new column where the class result labels are strings\n",
        "sal_df[\"ClassResult\"] = sal_df[\"Class Result\"].map({\n",
        "    0: \"True Positive\",\n",
        "    1: \"True Negative\",\n",
        "    2: \"False Positive\",\n",
        "    3: \"False Negative\"\n",
        "})\n",
        "hue_order = [\"True Positive\", \"False Positive\", \"True Negative\", \"False Negative\"]\n",
        "# bar plot- ratio in TP, TN, FP, FN for each mode\n",
        "p = sns.barplot(data=sal_df, x=\"modelname\", y=\"center ratio\", hue=\"ClassResult\", hue_order=hue_order)\n",
        "p.set(ylabel=\"Center vs. Outer Saliency Ratio\", title=\"Center vs. Outer Saliency Ratio by Class Result and Model\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, fmt='%.3f', padding=2)\n",
        "plt.show()\n",
        "\n",
        "# make new columns where the channel index is the color name\n",
        "sal_df[\"Center Important Channel\"] = sal_df[\"important channel center\"].map({\n",
        "    0: \"Red\",\n",
        "    1: \"Green\",\n",
        "    2: \"Blue\",\n",
        "})\n",
        "sal_df[\"Outer Important Channel\"] = sal_df[\"important channel outer\"].map({\n",
        "    0: \"Red\",\n",
        "    1: \"Green\",\n",
        "    2: \"Blue\",\n",
        "})\n",
        "hue_order = [\"Red\", \"Green\", \"Blue\"]\n",
        "palette = [\"red\", \"green\", \"blue\"]\n",
        "# plot important channel center by model\n",
        "p = sns.countplot(data=sal_df, x=\"modelname\", hue=\"Center Important Channel\", hue_order=hue_order, palette=palette)\n",
        "p.set(ylabel=\"Count of Images\", title=\"# Images by Model: Most Important Channel in Center\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, padding=2)\n",
        "plt.show()\n",
        "\n",
        "# plot important channel outer by model\n",
        "p = sns.countplot(data=sal_df, x=\"modelname\", hue=\"Outer Important Channel\", hue_order=hue_order, palette=palette)\n",
        "p.set(ylabel=\"Count of Images\", title=\"# Images by Model: Most Important Channel in Outer Area\")\n",
        "# p.tick_params(axis='x', rotation=30)\n",
        "for container in p.containers:\n",
        "  p.bar_label(container, padding=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rgq1B4ILZ_FV"
      },
      "id": "Rgq1B4ILZ_FV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PRlJJk1mKcay",
      "metadata": {
        "id": "PRlJJk1mKcay"
      },
      "outputs": [],
      "source": [
        "# representative images\n",
        "\n",
        "# pip install torch torchvision captum pillow matplotlib\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from captum.attr import Saliency\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# load internal test set images and run evaluation\n",
        "images,targets = next(iter(test_loader))\n",
        "images.to(device)\n",
        "idx = np.random.randint(0,len(images))\n",
        "model.eval()\n",
        "\n",
        "input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "output = model(input_tensor)\n",
        "pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "while targets[idx].item() != 0 and pred_class !=0:\n",
        "  idx = np.random.randint(0,len(images))\n",
        "  input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "input_tensor.requires_grad_()\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "  print(pred_class,targets[idx])\n",
        "  saliency = Saliency(model)\n",
        "  attr = saliency.attribute(input_tensor, target=pred_class)\n",
        "\n",
        "  attr = attr.abs().squeeze().cpu().detach().numpy()\n",
        "  attr = np.max(attr, axis=0)\n",
        "  print(attr.shape)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.imshow(np.array(images[idx].permute(2,1,0)), alpha=0.6)\n",
        "  plt.imshow(attr, cmap='hot', alpha=0.4)\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Predicted class: {pred_class} True class:{targets[idx].item()} {modelname}\")\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U38FyNjGqiLr",
      "metadata": {
        "id": "U38FyNjGqiLr"
      },
      "outputs": [],
      "source": [
        "# pip install torch torchvision captum pillow matplotlib\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from captum.attr import Saliency\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "images,targets = next(iter(external_test_loader))\n",
        "images.to(device)\n",
        "idx = np.random.randint(0,len(images))\n",
        "model.eval()\n",
        "\n",
        "input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "output = model(input_tensor)\n",
        "pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "while targets[idx].item() != 0 and pred_class !=0:\n",
        "  idx = np.random.randint(0,len(images))\n",
        "  input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "input_tensor.requires_grad_()\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "  print(pred_class,targets[idx])\n",
        "  saliency = Saliency(model)\n",
        "  attr = saliency.attribute(input_tensor, target=pred_class)\n",
        "\n",
        "  attr = attr.abs().squeeze().cpu().detach().numpy()\n",
        "  attr = np.max(attr, axis=0)\n",
        "  print(attr.shape)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.imshow(np.array(images[idx].permute(2,1,0)), alpha=0.6)\n",
        "  plt.imshow(attr, cmap='hot', alpha=0.4)\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Predicted class: {pred_class} True class:{targets[idx].item()} {modelname}\")\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ZZ-4W0v52zC",
      "metadata": {
        "id": "0ZZ-4W0v52zC"
      },
      "outputs": [],
      "source": [
        "# pip install torch torchvision captum pillow matplotlib\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from captum.attr import Saliency\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "images,targets = next(iter(test_loader))\n",
        "images.to(device)\n",
        "idx = np.random.randint(0,len(images))\n",
        "model.eval()\n",
        "\n",
        "input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "output = model(input_tensor)\n",
        "pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "while targets[idx].item() != 1 and pred_class !=1:\n",
        "  idx = np.random.randint(0,len(images))\n",
        "  input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "input_tensor.requires_grad_()\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "  print(pred_class,targets[idx])\n",
        "  saliency = Saliency(model)\n",
        "  attr = saliency.attribute(input_tensor, target=pred_class)\n",
        "\n",
        "  attr = attr.abs().squeeze().cpu().detach().numpy()\n",
        "  attr = np.max(attr, axis=0)\n",
        "  print(attr.shape)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.imshow(np.array(images[idx].permute(2,1,0)), alpha=0.6)\n",
        "  plt.imshow(attr, cmap='hot', alpha=0.4)\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Predicted class: {pred_class} True class:{targets[idx].item()} {modelname}\")\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37RMgRjgL0R3",
      "metadata": {
        "id": "37RMgRjgL0R3"
      },
      "outputs": [],
      "source": [
        "# pip install torch torchvision captum pillow matplotlib\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from captum.attr import Saliency\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "images,targets = next(iter(test_loader))\n",
        "images.to(device)\n",
        "idx = np.random.randint(0,len(images))\n",
        "model.eval()\n",
        "\n",
        "input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "output = model(input_tensor)\n",
        "pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "while targets[idx].item() == pred_class:\n",
        "  idx = np.random.randint(0,len(images))\n",
        "  input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "input_tensor = images[idx].to(device).unsqueeze(0)\n",
        "input_tensor.requires_grad_()\n",
        "\n",
        "for modelname, model in modeldict.items():\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  output = model(input_tensor)\n",
        "  pred_class = output.argmax(dim=1).item()\n",
        "  print(pred_class,targets[idx])\n",
        "  saliency = Saliency(model)\n",
        "  attr = saliency.attribute(input_tensor, target=pred_class)\n",
        "\n",
        "  attr = attr.abs().squeeze().cpu().detach().numpy()\n",
        "  attr = np.max(attr, axis=0)\n",
        "  print(attr.shape)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.imshow(np.array(images[idx].permute(2,1,0)), alpha=0.6)\n",
        "  plt.imshow(attr, cmap='hot', alpha=0.4)\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Predicted class: {pred_class} True class:{targets[idx].item()} {modelname}\")\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3uqNwP3COno-",
      "metadata": {
        "id": "3uqNwP3COno-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}