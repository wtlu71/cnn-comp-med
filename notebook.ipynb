{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b867135",
   "metadata": {
    "id": "0b867135"
   },
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837677e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7837677e",
    "outputId": "134d3198-89e5-47e3-a522-763f4200c3a7"
   },
   "outputs": [],
   "source": [
    "# installs- comment out if good\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "# imports\n",
    "from my_scripts.test import potato\n",
    "from my_scripts.my_models import SmallCNN, SmallMLP\n",
    "from my_scripts.dataset_loading import H5Dataset\n",
    "from my_scripts.utils import run_epoch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Path to your repo in Drive\n",
    "repo_path = '/content/drive/MyDrive/CompMed_CNN_Project/cnn-comp-med'\n",
    "# Add to Python path\n",
    "sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bdd56f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89bdd56f",
    "outputId": "b904c0bd-496f-47df-f026-ef33ff619270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(potato(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56a87d",
   "metadata": {
    "id": "9d56a87d"
   },
   "source": [
    "Get train/val/test data from Zenodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3870334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(),\"data\"),exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4ba219",
   "metadata": {
    "id": "ec4ba219"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1 -O data/train_x.h5.gz\n",
    "# !gunzip data/train_x.h5.gz\n",
    "\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1 -O data/train_y.h5.gz\n",
    "# !gunzip data/train_y.h5.gz\n",
    "\n",
    "# # val\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_x.h5.gz?download=1 -O data/valid_x.h5.gz\n",
    "# !gunzip data/valid_x.h5.gz\n",
    "\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_y.h5.gz?download=1 -O data/valid_y.h5.gz\n",
    "# !gunzip data/valid_y.h5.gz\n",
    "\n",
    "# # test\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1 -O data/test_x.h5.gz\n",
    "# !gunzip data/test_x.h5.gz\n",
    "\n",
    "# !wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_y.h5.gz?download=1 -O data/test_y.h5.gz\n",
    "# !gunzip data/test_y.h5.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577122da",
   "metadata": {
    "id": "577122da"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130649a2",
   "metadata": {
    "id": "130649a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transforms defined\n"
     ]
    }
   ],
   "source": [
    "# define transforms\n",
    "\n",
    "# is there a way to get this programmatically from the data or not worth it\n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation(10),\n",
    "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data transforms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a869510",
   "metadata": {
    "id": "2a869510"
   },
   "outputs": [],
   "source": [
    "# dataset paths- Colab virtual session\n",
    "train_img_h5_path = \"data/train_x.h5\"\n",
    "train_label_h5_path = \"data/train_y.h5\"\n",
    "\n",
    "val_img_h5_path = \"data/val_x.h5\"\n",
    "val_label_h5_path = \"data/val_y.h5\"\n",
    "\n",
    "test_img_h5_path = \"data/test_x.h5\"\n",
    "test_label_h5_path = \"data/test_y.h5\"\n",
    "\n",
    "train_subset_size = 50\n",
    "eval_subset_size = 10\n",
    "\n",
    "train_dataset = H5Dataset(train_img_h5_path,train_label_h5_path,transform=train_transforms)\n",
    "train_dataset_subset = Subset(train_dataset, range(train_subset_size))\n",
    "train_loader = DataLoader(train_dataset_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "val_dataset = H5Dataset(val_img_h5_path,val_label_h5_path,transform=train_transforms)\n",
    "val_dataset_subset = Subset(val_dataset, range(eval_subset_size))\n",
    "val_loader = DataLoader(val_dataset_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = H5Dataset(test_img_h5_path,test_label_h5_path,transform=train_transforms)\n",
    "test_dataset_subset = Subset(test_dataset, range(eval_subset_size))\n",
    "test_loader = DataLoader(test_dataset_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2f0bdd",
   "metadata": {
    "id": "ea2f0bdd"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ResNet50_Weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mlpmodel \u001b[38;5;241m=\u001b[39m SmallMLP()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m smallcnnmodel \u001b[38;5;241m=\u001b[39m SmallCNN()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m resnetmodel \u001b[38;5;241m=\u001b[39m resnet50(weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241m.\u001b[39mIMAGENET1K_V2)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# training hyperparameters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# can change learning rate or use scheduler\u001b[39;00m\n\u001b[1;32m      8\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-4\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ResNet50_Weights'"
     ]
    }
   ],
   "source": [
    "# define/load models\n",
    "mlpmodel = SmallMLP().to(device)\n",
    "smallcnnmodel = SmallCNN().to(device)\n",
    "resnetmodel = resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
    "\n",
    "# training hyperparameters\n",
    "# can change learning rate or use scheduler\n",
    "LEARNING_RATE = 3e-4\n",
    "# finetune with less epochs to avoid forgetting\n",
    "EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "# patience- number of epochs the model continues after no improvement in validation loss\n",
    "\n",
    "# consider other loss functions https://neptune.ai/blog/pytorch-loss-functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# don't change adam, never change dude\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fd1d2e",
   "metadata": {
    "id": "c1fd1d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/Users/rich/miniconda3/envs/geometric/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/rich/miniconda3/envs/geometric/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# TODO: Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     tr_loss, tr_acc, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# TODO: Validate\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     va_loss, va_acc, va_sens, va_spec, va_auc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, model, criterion, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Desktop/work/CMU/courses/Fall25/compmed/repo/cnn-comp-med/my_scripts/utils.py:29\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(loader, model, criterion, optimizer, train, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# TODO: Forward pass\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, targets)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# TODO: Backward pass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# do a backward pass to get gradients\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# do i do optimizer.zero_grad()?\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/work/CMU/courses/Fall25/compmed/repo/cnn-comp-med/my_scripts/my_models.py:75\u001b[0m, in \u001b[0;36mSmallMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# x = self.features(x)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# x = self.gap(x)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# # flatten until dimension 1 so that NxCx1x1 becomes NxC- suitable for linear layer\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# x = torch.flatten(x, 1)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# # print(x.shape)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# models = [mlpmodel, smallcnnmodel, resnetmodel]\n",
    "models = [mlpmodel, smallcnnmodel]\n",
    "\n",
    "historylist = []\n",
    "stopepochs = []\n",
    "for _, model in enumerate(models):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    # try to integrate into wandb instead of storing this so we can have a pretty dashboard?\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_auc\": []\n",
    "    }\n",
    "\n",
    "    best_auc = -np.inf\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        # TODO: Train for one epoch\n",
    "        tr_loss, tr_acc, _, _, _ = run_epoch(train_loader, model, criterion, optimizer=optimizer, train=True)\n",
    "\n",
    "        # TODO: Validate\n",
    "        va_loss, va_acc, va_sens, va_spec, va_auc = run_epoch(val_loader, model, criterion, optimizer=None, train=False,device=device)\n",
    "\n",
    "        # TODO: Store metrics\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "        history[\"val_auc\"].append(va_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: \"\n",
    "            f\"train_loss={tr_loss:.4f} \"\n",
    "            f\"val_loss={va_loss:.4f} \"\n",
    "            f\"val_acc={va_acc:.3f} \"\n",
    "            f\"val_auc={va_auc:.3f}\")\n",
    "\n",
    "        # TODO: Early stopping logic\n",
    "        if va_auc > best_auc + 1e-4:\n",
    "            # TODO: Update the best AUC\n",
    "            best_auc = va_auc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nRestored best model (val_auc={best_auc:.4f})\")\n",
    "\n",
    "    historylist.append(history)\n",
    "    stopepochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e442430",
   "metadata": {
    "id": "1e442430"
   },
   "outputs": [],
   "source": [
    "# finally, evaluate on test set\n",
    "\n",
    "for _, model in tqdm(enumerate(models)):\n",
    "    _, va_acc, va_sens, va_spec, va_auc = run_epoch(test_loader, model, criterion, train=False)\n",
    "    print(f\"\\nFinal Validation Performance:\")\n",
    "    print(f\"  AUC:         {va_auc:.4f}\")\n",
    "    print(f\"  Accuracy:    {va_acc:.4f}\")\n",
    "    print(f\"  Sensitivity: {va_sens:.4f}\")\n",
    "    print(f\"  Specificity: {va_spec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
