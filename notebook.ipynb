{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b867135",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7837677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs- comment out if good\n",
    "\n",
    "\n",
    "# imports\n",
    "from my_scripts.test import potato\n",
    "from my_scripts.my_models import SmallCNN, SmallMLP\n",
    "from my_scripts.dataset_loading import H5Dataset\n",
    "from my_scripts.utils import run_epoch\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bdd56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(potato(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56a87d",
   "metadata": {},
   "source": [
    "Get train/val/test data from Zenodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4ba219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# try to use the \"download all\" link and see what happens?\n",
    "\n",
    "# train\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1 -O train_subset_x.h5.gz\n",
    "!gunzip train_subset_x.h5.gz\n",
    "\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1 -O train_subset_y.h5.gz\n",
    "!gunzip train_subset_y.h5.gz\n",
    "\n",
    "# val\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_x.h5.gz?download=1 -O val_subset_x.h5.gz\n",
    "!gunzip val_subset_x.h5.gz\n",
    "\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_y.h5.gz?download=1 -O val_subset_y.h5.gz\n",
    "!gunzip val_subset_x.h5.gz\n",
    "\n",
    "# test\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1 -O test_subset_x.h5.gz\n",
    "!gunzip test_subset_x.h5.gz\n",
    "\n",
    "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_y.h5.gz?download=1 -O test_subset_y.h5.gz\n",
    "!gunzip test_subset_y.h5.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577122da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130649a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transforms defined\n"
     ]
    }
   ],
   "source": [
    "# define transforms\n",
    "\n",
    "# is there a way to get this programmatically from the data or not worth it\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data transforms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a869510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths- Colab virtual session\n",
    "train_img_h5_path = \"train_subset_x.h5\"\n",
    "train_label_h5_path = \"train_subset_y.h5\"\n",
    "\n",
    "val_img_h5_path = \"val_subset_x.h5\"\n",
    "val_label_h5_path = \"val_subset_y.h5\"\n",
    "\n",
    "test_img_h5_path = \"test_subset_x.h5\"\n",
    "test_label_h5_path = \"test_subset_y.h5\"\n",
    "\n",
    "train_subset_size = 50\n",
    "eval_subset_size = 10\n",
    "\n",
    "train_dataset = H5Dataset(train_img_h5_path,train_label_h5_path,transform=train_transforms)\n",
    "train_dataset_subset = Subset(train_dataset, range(train_subset_size))\n",
    "train_loader = DataLoader(train_dataset_subset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "val_dataset = H5Dataset(val_img_h5_path,val_label_h5_path,transform=train_transforms)\n",
    "val_dataset_subset = Subset(val_dataset, range(eval_subset_size))\n",
    "val_loader = DataLoader(val_dataset_subset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = H5Dataset(test_img_h5_path,test_label_h5_path,transform=train_transforms)\n",
    "test_dataset_subset = Subset(test_dataset, range(eval_subset_size))\n",
    "test_loader = DataLoader(test_dataset_subset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2f0bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\William/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:29<00:00, 3.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "# define/load models\n",
    "mlpmodel = SmallMLP().to(device)\n",
    "smallcnnmodel = SmallCNN().to(device)\n",
    "resnetmodel = resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
    "\n",
    "# training hyperparameters\n",
    "# can change learning rate or use scheduler\n",
    "LEARNING_RATE = 3e-4\n",
    "# finetune with less epochs to avoid forgetting\n",
    "EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "# patience- number of epochs the model continues after no improvement in validation loss\n",
    "\n",
    "# consider other loss functions https://neptune.ai/blog/pytorch-loss-functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# don't change adam, never change dude\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fd1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 416, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\William\\Documents\\GitHub\\cnn-comp-med\\my_scripts\\dataset_loading.py\", line 31, in __getitem__\n    self._open_file()\n    ~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\William\\Documents\\GitHub\\cnn-comp-med\\my_scripts\\dataset_loading.py\", line 20, in _open_file\n    self.img_h5_file = h5py.File(self.img_h5_path, \"r\")\n                       ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'train_subset_x.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# TODO: Train for one epoch\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     tr_loss, tr_acc, _, _, _ \u001b[38;5;241m=\u001b[39m run_epoch(train_loader, model, criterion, optimizer\u001b[38;5;241m=\u001b[39moptimizer, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# TODO: Validate\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     va_loss, va_acc, va_sens, va_spec, va_auc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, model, criterion, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\William\\Documents\\GitHub\\cnn-comp-med\\my_scripts\\utils.py:22\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, model, criterion, optimizer, train, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# TODO: Move to device\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1506\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1504\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n",
      "File \u001b[1;32mc:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1541\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data, worker_idx)\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1541\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\_utils.py:769\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 416, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\William\\Documents\\GitHub\\cnn-comp-med\\my_scripts\\dataset_loading.py\", line 31, in __getitem__\n    self._open_file()\n    ~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\William\\Documents\\GitHub\\cnn-comp-med\\my_scripts\\dataset_loading.py\", line 20, in _open_file\n    self.img_h5_file = h5py.File(self.img_h5_path, \"r\")\n                       ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"c:\\Users\\William\\anaconda3\\envs\\compmedhw2\\Lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'train_subset_x.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# models = [mlpmodel, smallcnnmodel, resnetmodel]\n",
    "models = [mlpmodel, smallcnnmodel]\n",
    "\n",
    "historylist = []\n",
    "stopepochs = []\n",
    "for _, model in enumerate(models):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    # try to integrate into wandb instead of storing this so we can have a pretty dashboard?\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_auc\": []\n",
    "    }\n",
    "\n",
    "    best_auc = -np.inf\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        # TODO: Train for one epoch\n",
    "        tr_loss, tr_acc, _, _, _ = run_epoch(train_loader, model, criterion, optimizer=optimizer, train=True)\n",
    "        \n",
    "        # TODO: Validate\n",
    "        va_loss, va_acc, va_sens, va_spec, va_auc = run_epoch(val_loader, model, criterion, optimizer=None, train=False)\n",
    "        \n",
    "        # TODO: Store metrics\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "        history[\"val_auc\"].append(va_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d}: \"\n",
    "            f\"train_loss={tr_loss:.4f} \"\n",
    "            f\"val_loss={va_loss:.4f} \"\n",
    "            f\"val_acc={va_acc:.3f} \"\n",
    "            f\"val_auc={va_auc:.3f}\")\n",
    "        \n",
    "        # TODO: Early stopping logic\n",
    "        if va_auc > best_auc + 1e-4:\n",
    "            # TODO: Update the best AUC\n",
    "            best_auc = va_auc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nRestored best model (val_auc={best_auc:.4f})\")\n",
    "    \n",
    "    historylist.append(history)\n",
    "    stopepochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e442430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, evaluate on test set\n",
    "\n",
    "for _, model in tqdm(enumerate(models)):\n",
    "    _, va_acc, va_sens, va_spec, va_auc = run_epoch(test_loader, model, criterion, train=False)\n",
    "    print(f\"\\nFinal Validation Performance:\")\n",
    "    print(f\"  AUC:         {va_auc:.4f}\")\n",
    "    print(f\"  Accuracy:    {va_acc:.4f}\")\n",
    "    print(f\"  Sensitivity: {va_sens:.4f}\")\n",
    "    print(f\"  Specificity: {va_spec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compmedhw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
