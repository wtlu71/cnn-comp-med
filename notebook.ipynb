{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b867135",
      "metadata": {
        "id": "0b867135"
      },
      "source": [
        "# Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7837677e",
      "metadata": {
        "id": "7837677e",
        "outputId": "134d3198-89e5-47e3-a522-763f4200c3a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# installs- comment out if good\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# to get our scripts\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Path to your repo in Drive\n",
        "repo_path = '/content/drive/MyDrive/Comp Med CNN Project/cnn-comp-med'\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.append(repo_path)\n",
        "\n",
        "# imports\n",
        "from my_scripts.test import potato\n",
        "from my_scripts.my_models import SmallCNN, SmallMLP\n",
        "from my_scripts.dataset_loading import H5Dataset\n",
        "from my_scripts.utils import run_epoch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet50\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JWDMtQAc7sqK"
      },
      "id": "JWDMtQAc7sqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89bdd56f",
      "metadata": {
        "id": "89bdd56f",
        "outputId": "b904c0bd-496f-47df-f026-ef33ff619270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(potato(2, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d56a87d",
      "metadata": {
        "id": "9d56a87d"
      },
      "source": [
        "Get train/val/test data from Zenodo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4ba219",
      "metadata": {
        "id": "ec4ba219"
      },
      "outputs": [],
      "source": [
        "# try to use the \"download all\" link and see what happens?\n",
        "\n",
        "# train\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1 -O train_subset_x.h5.gz\n",
        "!gunzip train_subset_x.h5.gz\n",
        "\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1 -O train_subset_y.h5.gz\n",
        "!gunzip train_subset_y.h5.gz\n",
        "\n",
        "# val\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_x.h5.gz?download=1 -O val_subset_x.h5.gz\n",
        "!gunzip val_subset_x.h5.gz\n",
        "\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_valid_y.h5.gz?download=1 -O val_subset_y.h5.gz\n",
        "!gunzip val_subset_x.h5.gz\n",
        "\n",
        "# test\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1 -O test_subset_x.h5.gz\n",
        "!gunzip test_subset_x.h5.gz\n",
        "\n",
        "!wget https://zenodo.org/records/2546921/files/camelyonpatch_level_2_split_test_y.h5.gz?download=1 -O test_subset_y.h5.gz\n",
        "!gunzip test_subset_y.h5.gz\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577122da",
      "metadata": {
        "id": "577122da"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130649a2",
      "metadata": {
        "id": "130649a2"
      },
      "outputs": [],
      "source": [
        "# define transforms\n",
        "\n",
        "# is there a way to get this programmatically from the data or not worth it\n",
        "IMG_SIZE = 96\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    #transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transforms (no augmentation)\n",
        "eval_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    # TODO: normalize: mean and std for ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"Data transforms defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a869510",
      "metadata": {
        "id": "2a869510"
      },
      "outputs": [],
      "source": [
        "# dataset paths- Colab virtual session\n",
        "train_img_h5_path = \"train_subset_x.h5\"\n",
        "train_label_h5_path = \"train_subset_y.h5\"\n",
        "\n",
        "val_img_h5_path = \"val_subset_x.h5\"\n",
        "val_label_h5_path = \"val_subset_y.h5\"\n",
        "\n",
        "test_img_h5_path = \"test_subset_x.h5\"\n",
        "test_label_h5_path = \"test_subset_y.h5\"\n",
        "\n",
        "train_subset_size = 50\n",
        "eval_subset_size = 10\n",
        "\n",
        "train_dataset = H5Dataset(train_img_h5_path,train_label_h5_path,transform=train_transforms)\n",
        "train_dataset_subset = Subset(train_dataset, range(train_subset_size))\n",
        "train_loader = DataLoader(train_dataset_subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "val_dataset = H5Dataset(val_img_h5_path,val_label_h5_path,transform=train_transforms)\n",
        "val_dataset_subset = Subset(val_dataset, range(eval_subset_size))\n",
        "val_loader = DataLoader(val_dataset_subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = H5Dataset(test_img_h5_path,test_label_h5_path,transform=train_transforms)\n",
        "test_dataset_subset = Subset(test_dataset, range(eval_subset_size))\n",
        "test_loader = DataLoader(test_dataset_subset, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2f0bdd",
      "metadata": {
        "id": "ea2f0bdd"
      },
      "outputs": [],
      "source": [
        "# define/load models\n",
        "mlpmodel = SmallMLP().to(device)\n",
        "smallcnnmodel = SmallCNN().to(device)\n",
        "resnetmodel = resnet50(weights = models.ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "\n",
        "# training hyperparameters\n",
        "# can change learning rate or use scheduler\n",
        "LEARNING_RATE = 3e-4\n",
        "# finetune with less epochs to avoid forgetting\n",
        "EPOCHS = 10\n",
        "PATIENCE = 5\n",
        "# patience- number of epochs the model continues after no improvement in validation loss\n",
        "\n",
        "# consider other loss functions https://neptune.ai/blog/pytorch-loss-functions\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# don't change adam, never change dude\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fd1d2e",
      "metadata": {
        "id": "c1fd1d2e"
      },
      "outputs": [],
      "source": [
        "# models = [mlpmodel, smallcnnmodel, resnetmodel]\n",
        "models = [mlpmodel, smallcnnmodel]\n",
        "\n",
        "historylist = []\n",
        "stopepochs = []\n",
        "for _, model in enumerate(models):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # try to integrate into wandb instead of storing this so we can have a pretty dashboard?\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"val_auc\": []\n",
        "    }\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_state = None\n",
        "    bad_epochs = 0\n",
        "\n",
        "    print(\"\\nStarting training...\\n\")\n",
        "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
        "        # TODO: Train for one epoch\n",
        "        tr_loss, tr_acc, _, _, _ = run_epoch(train_loader, model, criterion, optimizer=optimizer, train=True)\n",
        "\n",
        "        # TODO: Validate\n",
        "        va_loss, va_acc, va_sens, va_spec, va_auc = run_epoch(val_loader, model, criterion, optimizer=None, train=False)\n",
        "\n",
        "        # TODO: Store metrics\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"val_loss\"].append(va_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "        history[\"val_auc\"].append(va_auc)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}: \"\n",
        "            f\"train_loss={tr_loss:.4f} \"\n",
        "            f\"val_loss={va_loss:.4f} \"\n",
        "            f\"val_acc={va_acc:.3f} \"\n",
        "            f\"val_auc={va_auc:.3f}\")\n",
        "\n",
        "        # TODO: Early stopping logic\n",
        "        if va_auc > best_auc + 1e-4:\n",
        "            # TODO: Update the best AUC\n",
        "            best_auc = va_auc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            bad_epochs = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= PATIENCE:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f\"\\nRestored best model (val_auc={best_auc:.4f})\")\n",
        "\n",
        "    historylist.append(history)\n",
        "    stopepochs.append(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e442430",
      "metadata": {
        "id": "1e442430"
      },
      "outputs": [],
      "source": [
        "# finally, evaluate on test set\n",
        "\n",
        "for _, model in tqdm(enumerate(models)):\n",
        "    _, va_acc, va_sens, va_spec, va_auc = run_epoch(test_loader, model, criterion, train=False)\n",
        "    print(f\"\\nFinal Validation Performance:\")\n",
        "    print(f\"  AUC:         {va_auc:.4f}\")\n",
        "    print(f\"  Accuracy:    {va_acc:.4f}\")\n",
        "    print(f\"  Sensitivity: {va_sens:.4f}\")\n",
        "    print(f\"  Specificity: {va_spec:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}